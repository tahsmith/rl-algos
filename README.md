# Reinforcement Learning

## Tabular Methods

These methods sequentially approximate the value of taking an action in each 
states, (the q function,) as a table `Q[state, value]`. The 'value' here
means the expected reward if the agent continues to follow the current policy 
after the current experience.

Here I have written them in a very functional form so that they look like they
do in the textbooks as much as possible. Each algorithm is implemented as a 
"episode function" which take initial state, a 'step' function which gives, 
subsequent states for each action, the current iteration number, `i`, and the
current value of the Q table. Then they will return an updated value of the 
Q table after experiencing one episode.

The episode functions are generated by a factory function that allows things 
like the policy uses and the decay of the learning rate and other functions to 
be customised without affecting the clarity of the core implementation.

This stateless approach has been useful in other ways:

 * easy to introduce logging and tracing without cluttering the core algo
 * can plot everything because everything is a simple function.
 * doesn't rely on any fancy features of the language that would not be 
 available elsewhere. I can translate the algos to any other system easily.

Currenly implemented:

 * sarsa, sarsa lambda
 * a choice of epsilon-greedy or softmax exploration policies.


## Policy gradient

Work in progress.

 * reinforce
 * ppo