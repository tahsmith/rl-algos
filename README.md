# Reinforcement Learning

## Tabular Methods

These methods sequentially approximate the value of taking an action in each 
states, (the q function,) as a table `Q[state, value]`. The 'value' here
means the expected reward if the agent continues to follow the current policy 
after the current experience.

Here I have written them in a very functional form so that they look like they
do in the textbooks as much as possible. Each algorithm is implemented as a 
"episode function" which take initial state, a 'step' function which gives, 
subsequent states for each action, the current iteration number, `i`, and the
current value of the Q table. Then they will return an updated value of the 
Q table after experiencing one episode.

The episode functions are generated by a factory function that allows things 
like the policy uses and the decay of the learning rate and other functions to 
be customised without affecting the clarity of the core implementation.

This stateless approach has been useful in other ways:

 * easy to introduce logging and tracing without cluttering the core algo
 * can plot everything because everything is a simple function.
 * doesn't rely on any fancy features of the language that would not be 
 available elsewhere. I can translate the algos to any other system easily.

Currenly implemented:

 * sarsa, sarsa lambda, q-learning, dyna-q
 * a choice of epsilon-greedy or softmax exploration policies.
 
*Terminal states*: The value of terminal states are never set to zero. They 
will always sit at their initial values. This is a bit different to other 
presentations of these algos, but it makes them a bit nicer when working with
 OpenAI gym. Episodes of some of the games often end at non-terminal states, 
 i.e., receiving "done" does not necessarily mean that the state was terminal.


## Policy gradient

Work in progress.

 * reinforce
 * ppo